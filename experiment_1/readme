WORKSPACE en donde se genera el dataset utilizado
en el "experimento 2"; el cual filtra los mejores
modelos y decide como va a valer la pena variar
los hiper-parametros en el experimento 3; el cual
genera el dataset con el cual se fabrican los resultados.

Para generarlo se puede:

       A) correr "main.py" y esperar 
          a que termine el loop de 1000
          casos (1 dia) o cortarlo cuando 
          quieras dado que en cada iteracion
          genera y guarda los resultados del
          modelo en "results.csv".

       B) generar el docker container con 
          "docker-compose up -d" y el proceso
          va a ser infinito en el sentido de 
          que va a realizar 5 iteraciones, se
          va a terminar, y va a tener auto-restart
          y el motivo por el cual es asi es porque
          no me funcionaba "backend.clear_session()"
          entonces se quedaba sin memoria y usaba 
          SWAP. 
          El beneficio es que esta setteado para
          usar 25% de la CPU con 4 cores; entonces
          puede quedar de fondo e incluso configurarse
          para que use menos. Ademas usa Tensorflow 
          optimizado para Intel Cores.
           
